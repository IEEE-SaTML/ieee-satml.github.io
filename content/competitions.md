title: Competitions
menu_title: Competitions
template: page
menu_order: 300

## Competition Track

SaTML traditionally includes a *Competition Track* as part of its program. Participants are invited to engage in selected data science competitions, competing to achieve the highest score on relevant machine learning or security tasks. These tasks are based on well-defined problems and corresponding datasets provided by the competition organizers. The competition results will be presented and discussed during dedicated sessions at the conference (see [Call for Competitions](/call-for-competitions) for details). 

## Accepted Competitions

For this year, the following competitions have been accepted for the conference. Interested researchers can participate in any of these by following the instructions provided on the competition websites. For more information or specific inquiries, please contact the respective competition organizers directly.

<a class="anchor" name="competition1"></a>
### üèÅ Detecting Manipulations of AI Models in Space Operations

Website: <https://assurance-ai.space-codev.org/competitions/>

> The competition is a part of the "Assurance for Space Domain AI Applications" project funded by the European Space Agency. It looks for effective algorithms to identify security issues in AI models across two real-life space operation scenarios: 1) manipulated outputs from LLM-based summarization of space-related texts ("Impostor Hunt") and 2) hidden triggers in models for spacecraft telemetry forecasting ("Trojan Horse Hunt").

Organizers: Agata Kaczmarek, Dawid P≈Çudowski, Piotr Wilczy≈Ñski, Przemys≈Çaw Biecek, Artur Janicki, Krzysztof Kotowski, Ramez Shendy, Jakub Nalepa, and Evridiki Ntagiou

<a class="anchor" name="competition2"></a>
### üèÅ Anti-BAD: An Anti-Backdoor Challenge for Post-Trained Large Language Models

Website: <https://anti-bad.github.io/>

> This competition invites participants to defend against backdoors in large language models under practical deployment constraints (i.e., without access to training data or poisoned prior knowledge). Spanning generation, classification, and multilingual tracks, Anti-BAD encourages lightweight yet effective defenses that restore model integrity while preserving utility in practical post-trained scenarios common to model-sharing ecosystems.

Organizers: Weijun Li, Jinrui Yang, Ansh Arora, Yiyi Chen, Xuanli He, Heather Lent, Johannes Bjerva, Mark Dras, and Qiongkai Xu

<a class="anchor" name="competition3"></a>
### üèÅ Agentic System Capture-the-Flag Competition

Website: <https://ctf.secure-agent.com/>

> AgentCTF is a security-focused competition designed to evaluate and benchmark agentic systems through adversarial and defensive challenges. Participants deploy agents in realistic, CVE-inspired environments across red-teaming and attack-defense tracks, testing vulnerabilities, defenses, and adaptive strategies. Built on an online evaluation platform, the competition integrates established benchmarks, ensures standardized evaluation, and fosters community engagement.

Organizers: Berkeley RDI Center

<a class="anchor" name="competition4"></a>
### üèÅ TBD

Website: <https://TBD>

> TBD

Organizers: TBD